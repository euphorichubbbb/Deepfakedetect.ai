{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import datasets, layers, models\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "labels = ('real', 'fake')\n",
    "def get_data(data_dir):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img))[..., ::-1]\n",
    "                resized_arr = cv2.resize(img_arr, (image_size, image_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    return np.array(data, dtype='object')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using above func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 2)\n",
      "(720, 2)\n"
     ]
    }
   ],
   "source": [
    "train = get_data(r\"C:\\Users\\91636\\Desktop\\aventus_final\\vf\\real_vs_fake\\train\")\n",
    "test = get_data(r\"C:\\Users\\91636\\Desktop\\aventus_final\\vf\\real_vs_fake\\test\")\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "l = []\n",
    "for i in train:\n",
    "    if i[1] == 0:\n",
    "        l.append('real')\n",
    "    else:\n",
    "        l.append('fake')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorizing data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# dividing data into train and test sets\n",
    "for feature, label in train:\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in test:\n",
    "    X_test.append(feature)\n",
    "    y_test.append(label)\n",
    "\n",
    "# Normalise the image data (to convert it to range from 0 to 1)\n",
    "X_train = np.array(X_train)/ 255.0\n",
    "\n",
    "X_test = np.array(X_test)/ 255.0\n",
    "\n",
    "# # Reshaping the image data so that every image is the same size\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_train= tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "y_test=tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "print (X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn model initialisation and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12/12 [==============================] - 39s 3s/step - loss: 0.9183 - accuracy: 0.4700\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 33s 3s/step - loss: 0.6936 - accuracy: 0.4658\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 33s 3s/step - loss: 0.6932 - accuracy: 0.4958\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 33s 3s/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 33s 3s/step - loss: 0.6932 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d1ecef0a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "cnn.fit(X_train,y_train, epochs = 5,batch_size=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"vit_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "CATEGORIES = [\"REAL\",\"FAKE\"]\n",
    "\n",
    "\n",
    "# def prepare(filepath):\n",
    "#     img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)  # read in the image, convert to grayscale\n",
    "#     new_array = cv2.resize(img_array, (64,64))  # resize image to match model's expected sizing\n",
    "#     return new_array.reshape(-1,64,64,3)  # return the image with shaping that TF wants.\n",
    "def create_dataset(file_path):\n",
    " \n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "    \n",
    "\n",
    "      \n",
    "            \n",
    "            # Get the complete video path.\n",
    "            # print(file_path)\n",
    "            # Extract the frames of the video file.\n",
    "    frames = cv2.imread(file_path)\n",
    "    frames = cv2.resize(frames, (224,224))\n",
    "            # print(frames)\n",
    "            # Check if the extracted frames are equal to ,the SEQUENCE_LENGTH specified.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            \n",
    " \n",
    "                # Append the data to their repective lists.\n",
    "    features.append(frames)\n",
    " \n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)  \n",
    "\n",
    "    return features\n",
    "model =keras.models.load_model(r\"D:\\JITESH\\aventus\\vision_transformer\\vit_final.h5\",compile=False)\n",
    "# pickled_model = pickle.load(\"model11.pkl\")\n",
    "# pickled_model = pickle.load(open('model11.pkl', 'rb'))\n",
    "# model = tf.keras.models.load_model(\"CNN1.h5\", compile=False)\n",
    "prediction = model.predict([create_dataset(r\"C:\\Users\\91636\\Desktop\\aventus_final\\vf\\real_vs_fake\\test\\real\\00001.jpg\")]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5165581 , 0.48344195]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REAL'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction1 = prediction.tolist()\n",
    "prediction_list = max(prediction1)\n",
    "max_ele=max(prediction_list)\n",
    "index=prediction_list.index(max_ele)\n",
    "CATEGORIES[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
